{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Web Scraping and NLP with Requests, BeautifulSoup, and spaCy\n",
    "\n",
    "### Student Name: Anthony Schomer \n",
    "### https://github.com/anythonyschomer/web-scraping\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write code that extracts the article html from https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/ and dumps it to a .pkl (or other appropriate file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (989057474.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 23\u001b[1;36m\u001b[0m\n\u001b[1;33m    import pickle\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Web Scraping and NLP with Requests, BeautifulSoup, and spaCy\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "# Make a GET request to the URL\n",
    "url = 'https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Extract the HTML content from the response\n",
    "    html_content = response.text\n",
    "\n",
    "    # Dump the HTML content into a pickle file\n",
    "    with open('article.pkl', 'wb') as f:\n",
    "        pickle.dump(html_content, f)\n",
    "        print('Article HTML dumped into \"article.pkl\" file.')\n",
    "else:\n",
    "    print('Failed to retrieve article HTML.')\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "   import pickle\n",
    "   import requests\n",
    "   import spaCy\n",
    "   from bs4 import BeautifulSoup\n",
    "   import matplotlib.pyplot as plt\n",
    "   \n",
    "   \n",
    "   \n",
    "    # Using the Counter class from collections module\n",
    "   my_list = [1, 2, 3, 3, 3, 4, 4, 5]\n",
    "   counter = Counter(my_list)\n",
    "   print(counter)\n",
    "\n",
    "   # Using the pickle module\n",
    "   data = {'name': 'John', 'age': 30}\n",
    "   with open('data.pkl', 'wb') as f:\n",
    "       pickle.dump(data, f)\n",
    "\n",
    "   # Using the requests module\n",
    "   response = requests.get('https://example.com')\n",
    "   print(response.status_code)\n",
    "\n",
    "   # Using the spacy library\n",
    "   nlp = spaCy.load('en_core_web_sm')\n",
    "   doc = nlp('This is an example sentence')\n",
    "   print([token.text for token in doc])\n",
    "\n",
    "   # Using the BeautifulSoup class from bs4 module\n",
    "   html='Title'\n",
    "   soup = BeautifulSoup(html,'html.parser')\n",
    "   print(soup.title.text)\n",
    "\n",
    "   # Using the pyplot module from matplotlib\n",
    "   x = [1, 2, 3, 4, 5]\n",
    "   y = [1, 4, 9, 16, 25]\n",
    "   plt.plot(x, y)\n",
    "   plt.show()\n",
    "   \n",
    "   !pip list\n",
    "\n",
    "print('All prereqs installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and print it's text (use `.get_text()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'article.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      4\u001b[0m \u001b[39m# Read the HTML content from the pickle file\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39marticle.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     html_content \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Create a BeautifulSoup object for parsing the HTML\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'article.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the HTML content from the pickle file\n",
    "with open('article.pkl', 'rb') as f:\n",
    "    html_content = pickle.load(f)\n",
    "\n",
    "# Create a BeautifulSoup object for parsing the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the text from the HTML using .get_text()\n",
    "text = soup.get_text()\n",
    "\n",
    "# Print the extracted text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39men\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstop_words\u001b[39;00m \u001b[39mimport\u001b[39;00m STOP_WORDS\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the article text\n",
    "article_text = \"Your article text goes here.\"\n",
    "\n",
    "# Load the spaCy English pipeline\n",
    "nlp = spaCy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the article text using the spaCy pipeline\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Tokenize the text, convert to lowercase, and filter out unwanted tokens\n",
    "tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_space and not token.is_stop]\n",
    "\n",
    "# Get the most frequent tokens\n",
    "most_common_tokens = Counter(tokens).most_common(5)\n",
    "\n",
    "# Print the most common tokens\n",
    "print(\"5 Most Frequent Tokens:\")\n",
    "for token, frequency in most_common_tokens:\n",
    "    print(token, \"-\", frequency)\n",
    "\n",
    "# Get all tokens with their frequencies\n",
    "all_token_frequencies = Counter(tokens)\n",
    "\n",
    "# Print all tokens with their frequencies\n",
    "print(\"\\nToken Frequencies:\")\n",
    "for token, frequency in all_token_frequencies.items():\n",
    "    print(token, \"-\", frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39men\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstop_words\u001b[39;00m \u001b[39mimport\u001b[39;00m STOP_WORDS\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the article text\n",
    "article_text = \"Your article text goes here.\"\n",
    "\n",
    "# Load the spaCy English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the article text using the spaCy pipeline\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Get the lemmas of the tokens, convert to lowercase, and filter out unwanted lemmas\n",
    "lemmas = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space and not token.is_stop]\n",
    "\n",
    "# Get the most frequent lemmas\n",
    "most_common_lemmas = Counter(lemmas).most_common(5)\n",
    "\n",
    "# Print the most common lemmas\n",
    "print(\"5 Most Frequent Lemmas:\")\n",
    "for lemma, frequency in most_common_lemmas:\n",
    "    print(lemma, \"-\", frequency)\n",
    "\n",
    "# Get all lemmas with their frequencies\n",
    "all_lemma_frequencies = Counter(lemmas)\n",
    "\n",
    "# Print all lemmas with their frequencies\n",
    "print(\"\\nLemma Frequencies:\")\n",
    "for lemma, frequency in all_lemma_frequencies.items():\n",
    "    print(lemma, \"-\", frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define the following methods:\n",
    "    * `score_sentence_by_token(sentence, interesting_token)` that takes a sentence and a list of interesting token and returns the number of times that any of the interesting words appear in the sentence divided by the number of words in the sentence\n",
    "    * `score_sentence_by_lemma(sentence, interesting_lemmas)` that takes a sentence and a list of interesting lemmas and returns the number of times that any of the interesting lemmas appear in the sentence divided by the number of words in the sentence\n",
    "    \n",
    "You may find some of the code from the in class notes useful; feel free to use methods (rewrite them in this cell as well).  Test them by showing the score of the first sentence in your article using the frequent tokens and frequent lemmas identified in question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence_by_token(sentence, interesting_tokens):\n",
    "    # Initialize the count\n",
    "    count = 0\n",
    "    \n",
    "    # Split the sentence into tokens\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    # Calculate the count of interesting tokens in the sentence\n",
    "    for token in tokens:\n",
    "        if token.lower() in interesting_tokens:\n",
    "            count += 1\n",
    "            \n",
    "    # Calculate the score\n",
    "    score = count / len(tokens)\n",
    "    \n",
    "    # Return the score\n",
    "    return score\n",
    "\n",
    "\n",
    "def score_sentence_by_lemma(sentence, interesting_lemmas):\n",
    "    # Initialize the count\n",
    "    count = 0\n",
    "    \n",
    "    # Load the spaCy English pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Process the sentence using the spaCy pipeline\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Get the lemmas of the tokens and convert to lowercase\n",
    "    lemmas = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space and not token.is_stop]\n",
    "    \n",
    "    # Calculate the count of interesting lemmas in the sentence\n",
    "    for lemma in lemmas:\n",
    "        if lemma in interesting_lemmas:\n",
    "            count += 1\n",
    "            \n",
    "    # Calculate the score\n",
    "    score = count / len(lemmas)\n",
    "    \n",
    "    # Return the score\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Which tokens and lexems would be ommitted from the lists generated in questions 3 and 4 if we only wanted to consider nouns as interesting words?  How might we change the code to only consider nouns? Put your answer in this Markdown cell (you can edit it by double clicking it)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
